{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyreadr\n",
    "from numpy import loadtxt\n",
    "from numpy import sort\n",
    "from xgboost import XGBClassifier,XGBRFClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_validate, KFold\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, precision_score, recall_score, confusion_matrix,roc_curve\n",
    "from sklearn.feature_selection import SelectFromModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in feature_selection_params*.csv\n",
    "\n",
    "version = 1\n",
    "features = pd.read_csv(f'feature_selection_params{version}.csv'.format(version), index_col=0)\n",
    "print(f'feature_selection_params{version}.csv')\n",
    "#order by number of features\n",
    "features = features.sort_values('number_of_features', ascending=True)\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the elbow point using the kneedle algorithm\n",
    "from kneed import KneeLocator\n",
    "#find the knee point for each feature selection method\n",
    "# kneedle =KneeLocator(x=features[\"number_of_features\"],y=features['AUCs'], S=5,curve='concave', direction='increasing', interp_method='polynomial')\n",
    "kneedle = KneeLocator(x=features[\"number_of_features\"][1:200],y=features['AUCs'][1:200],S=5,curve='concave', direction='increasing', interp_method='polynomial')\n",
    "print(kneedle.knee)\n",
    "# print(kneedle2.knee)\n",
    "\n",
    "#plot the knee point\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(features['number_of_features'], features['AUCs'])\n",
    "plt.axvline(kneedle.knee, color='red')\n",
    "#label the knee point\n",
    "plt.text(kneedle.knee+10, 0.85, f\"knee = {kneedle.knee}\")\n",
    "# plt.axvline(kneedle2.knee, color='green')\n",
    "plt.xlabel('Number of features')\n",
    "plt.ylabel('AUC')\n",
    "plt.title(f\"Feature selection method {version} AUC vs number of features\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_xgboost_model( X_train, y_train, n_splits=10, suppress_output=False):\n",
    "    params = {\"objective\": \"binary:logistic\", \n",
    "                \"eval_metric\": \"auc\", \n",
    "                \"eta\":0.1, \n",
    "                \"max_depth\":20,\n",
    "                \"lambda\": 0.0003, \"alpha\": 0.0003, \"nthread\" :10}\n",
    "    model = XGBClassifier(**params) \n",
    "\n",
    "\n",
    "    kf = KFold(n_splits=n_splits, random_state=6, shuffle=True)\n",
    "    \n",
    "    #store the accuracy and auc for each fold\n",
    "    accuracy_list = []\n",
    "    roc_auc_list = []\n",
    "    recall_list = []\n",
    "    specificity_list = []\n",
    "    best_model = None\n",
    "\n",
    "    for fold, (train_index, valid_index) in enumerate(kf.split(X_train)):\n",
    "        X_train_fold, X_valid_fold = X_train.iloc[train_index], X_train.iloc[valid_index]\n",
    "        y_train_fold, y_valid_fold = y_train.iloc[train_index], y_train.iloc[valid_index]\n",
    "        model.fit(X_train_fold, y_train_fold.values)\n",
    "        y_pred = model.predict(X_valid_fold)\n",
    "        y_pred_proba = model.predict_proba(X_valid_fold)[:,1]\n",
    "        accuracy = accuracy_score(y_valid_fold, y_pred)\n",
    "        roc_auc = roc_auc_score(y_valid_fold, y_pred_proba)\n",
    "        f1 = f1_score(y_valid_fold, y_pred)\n",
    "        precision = precision_score(y_valid_fold, y_pred)\n",
    "        \n",
    "        tn, fp, fn, tp = confusion_matrix(y_valid_fold, y_pred).ravel()\n",
    "        specificity = tn / (tn+fp)\n",
    "\n",
    "        recall = recall_score(y_valid_fold, y_pred)\n",
    "\n",
    "\n",
    "        if roc_auc > max(roc_auc_list, default=0):\n",
    "            best_model = model\n",
    "            fpr, tpr, thresholds = roc_curve(y_valid_fold, y_pred_proba)\n",
    "        accuracy_list.append(accuracy)\n",
    "        roc_auc_list.append(roc_auc)\n",
    "        recall_list.append(recall)\n",
    "        specificity_list.append(specificity)\n",
    "\n",
    "\n",
    "\n",
    "        if not suppress_output:\n",
    "            print(f\"Fold: {fold}, Accuracy: {accuracy}, ROC AUC: {roc_auc}, F1: {f1}, Precision: {precision}, Recall: {recall}, Specificity: {specificity}\")\n",
    "\n",
    "            #draw the ROC curve\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC curve')\n",
    "    #add label\n",
    "    plt.text(0.6, 0.2, f\"AUC = {roc_auc}\")\n",
    "        \n",
    "    return {\"model\": best_model, \"accuracy\": accuracy_list, \"roc_auc\": roc_auc_list, \"recall\": recall_list, \"specificity\": specificity_list, \"fpr\": fpr, \"tpr\": tpr}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filename = f\"Rdata/external_training_test_{version}.rda\"\n",
    "\n",
    "training_test = pyreadr.read_r(filename)\n",
    "train1 = training_test[\"um_data\"]\n",
    "test1 = training_test[\"external_data\"]\n",
    "\n",
    "\n",
    "print(f\"train1 shape: {train1.shape}, test1 shape: {test1.shape}\")\n",
    "\n",
    "X1_train=train1.drop([\"Row.names\",\"ALS_status\"],axis=1)\n",
    "y1_train=train1[\"ALS_status\"]\n",
    "y1_train=y1_train.replace({'case':1,'control':0})\n",
    "\n",
    "X1_test=test1.drop([\"Row.names\",\"ALS_status\"],axis=1)\n",
    "y1_test=test1[\"ALS_status\"]\n",
    "#count how many case and control in ALS_status\n",
    "y1_test=y1_test.replace({'case':1,'control':0})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "whole_results = cross_validate_xgboost_model(X1_train, y1_train, n_splits=10)\n",
    "best_model_whole = whole_results[\"model\"]\n",
    "#print the average accuracy and roc_auc\n",
    "print(f\"Average accuracy: {sum(whole_results['accuracy'])/len(whole_results['accuracy'])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "thresholds = sort(best_model_whole.feature_importances_)\n",
    "thresh= thresholds[-kneedle.knee]\n",
    "# thresh2 = thresholds[-kneedle2.knee]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select features using threshold\n",
    "selection = SelectFromModel(estimator=best_model_whole, threshold=thresh, prefit=True)\n",
    "col_index = X1_train.columns[selection.get_support()]\n",
    "select_X_train = selection.transform(X1_train)\n",
    "select_X_train = pd.DataFrame(select_X_train, columns=col_index)\n",
    "print(f\"thresh: {thresh}, n={select_X_train.shape[1]}\")\n",
    "# train model\n",
    "results = cross_validate_xgboost_model(select_X_train, y1_train, n_splits=10, suppress_output=True)\n",
    "#get the average of accuracy and auc\n",
    "accuracy = sum(results['accuracy'])/len(results['accuracy'])\n",
    "auc = sum(results['roc_auc'])/len(results['roc_auc'])\n",
    "sensitivity = sum(results['recall'])/len(results['recall'])\n",
    "specificity = sum(results['specificity'])/len(results['specificity'])\n",
    "\n",
    "fpr, tpr = results['fpr'], results['tpr']\n",
    "\n",
    "print(f\"accuracy: {accuracy*100:.2f}%, sensitivity: {sensitivity*100:.2f}%, specificity: {specificity*100:.2f}%, AUC: {auc*100:.2f}%\")\n",
    "print(\"--------------------------------------------------\")\n",
    "\n",
    "#save the model\n",
    "import joblib\n",
    "joblib.dump(results['model'], f\"model_{version}.joblib\")\n",
    "features = results['model'].get_booster().feature_names\n",
    "# Save selected features to txt file\n",
    "with open(f\"model_{version}_features.txt\", \"w\") as f:\n",
    "    for feature in features:\n",
    "        f.write(f\"{feature}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_model(results, X_test, y_test):\n",
    "    model = results[\"model\"]\n",
    "\n",
    "    selected_features = model.get_booster().feature_names\n",
    "    test_ds = X_test[selected_features]\n",
    "    print(f\"test_ds shape: {test_ds.shape}\")\n",
    "\n",
    "    y_pred = model.predict(test_ds)\n",
    "    print(y_pred)\n",
    "    y_pred_proba = model.predict_proba(test_ds)[:,1]\n",
    "\n",
    "    #draw the roc curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    plt.plot(fpr, tpr, label=\"external test\")\n",
    "    plt.plot(results[\"fpr\"], results[\"tpr\"], label=\"internal CV\")\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC curve')\n",
    "    #add label\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    \n",
    "    #show the AUC by label\n",
    "    plt.legend([f\"external AUC: {roc_auc_score(y_test, y_pred_proba)*100:.2f}%\", f\"internal CV AUC: {sum(results['roc_auc'])/len(results['roc_auc'])*100:.2f}%\"])  \n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    specificity = tn / (tn+fp)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    \n",
    "    #return in percentage keep 2 decimal\n",
    "    return {\"accuracy\": accuracy*100, \"roc_auc\": roc_auc*100, \"f1\": f1*100, \"precision\": precision*100, \"recall\": recall*100, \"specificity\": specificity*100}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "external_results = predict_model(results, X1_test, y1_test)\n",
    "print(f\"External test accuracy: {external_results['accuracy']:2f}%, roc_auc: {external_results['roc_auc']:.2f}%, sensitivity: {external_results['recall']:.2f}%, specificity: {external_results['specificity']:.2f}%, auc: {external_results['roc_auc']:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "feature_importance = results[\"model\"].get_booster().get_score(importance_type='weight')\n",
    "feature_importance = pd.DataFrame(feature_importance.items(), columns=['feature', 'weight'])\n",
    "\n",
    "#merge the feature importance with the gene symbol\n",
    "feature_importance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
