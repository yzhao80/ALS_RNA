{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyreadr\n",
    "from numpy import loadtxt\n",
    "from numpy import sort\n",
    "from xgboost import XGBClassifier,XGBRFClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_validate, KFold\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, precision_score, recall_score, confusion_matrix,roc_curve\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "#plot the knee point\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "versions = [1,3,4]\n",
    "n_features = [27,29,30]\n",
    "colors={1:\"#F28147\", 3:\"#9FD4AE\",4:\"#5560AC\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_xgboost_model( X_train, y_train, n_splits=10, suppress_output=False, validation_set_prediction = False):\n",
    "    params = {\"objective\": \"binary:logistic\", \n",
    "                \"eval_metric\": \"auc\", \n",
    "                \"eta\":0.1, \n",
    "                \"max_depth\":20,\n",
    "                \"lambda\": 0.0003, \"alpha\": 0.0003, \"nthread\" :10}\n",
    "    model = XGBClassifier(**params) \n",
    "\n",
    "\n",
    "    kf = KFold(n_splits=n_splits, random_state=6, shuffle=True)\n",
    "    \n",
    "    #store the accuracy and auc for each fold\n",
    "    accuracy_list = []\n",
    "    roc_auc_list = []\n",
    "    recall_list = []\n",
    "    specificity_list = []\n",
    "    best_model = None\n",
    "\n",
    "    for fold, (train_index, valid_index) in enumerate(kf.split(X_train)):\n",
    "        X_train_fold, X_valid_fold = X_train.iloc[train_index], X_train.iloc[valid_index]\n",
    "        y_train_fold, y_valid_fold = y_train.iloc[train_index], y_train.iloc[valid_index]\n",
    "        model.fit(X_train_fold, y_train_fold.values)\n",
    "        y_pred = model.predict(X_valid_fold)\n",
    "        y_pred_proba = model.predict_proba(X_valid_fold)[:,1]\n",
    "        accuracy = accuracy_score(y_valid_fold, y_pred)\n",
    "        roc_auc = roc_auc_score(y_valid_fold, y_pred_proba)\n",
    "        f1 = f1_score(y_valid_fold, y_pred)\n",
    "        precision = precision_score(y_valid_fold, y_pred)\n",
    "        \n",
    "        tn, fp, fn, tp = confusion_matrix(y_valid_fold, y_pred).ravel()\n",
    "        specificity = tn / (tn+fp)\n",
    "\n",
    "        recall = recall_score(y_valid_fold, y_pred)\n",
    "        if validation_set_prediction:\n",
    "            valid_set_predict = pd.concat([\n",
    "                valid_set_predict,\n",
    "                pd.DataFrame(y_pred_proba, index=y_valid_fold.index)\n",
    "            ], axis=0)\n",
    "\n",
    "\n",
    "        if roc_auc > max(roc_auc_list, default=0):\n",
    "            best_model = model\n",
    "            fpr, tpr, thresholds = roc_curve(y_valid_fold, y_pred_proba)\n",
    "        accuracy_list.append(accuracy)\n",
    "        roc_auc_list.append(roc_auc)\n",
    "        recall_list.append(recall)\n",
    "        specificity_list.append(specificity)\n",
    "\n",
    "\n",
    "\n",
    "        if not suppress_output:\n",
    "            print(f\"Fold: {fold}, Accuracy: {accuracy}, ROC AUC: {roc_auc}, F1: {f1}, Precision: {precision}, Recall: {recall}, Specificity: {specificity}\")\n",
    "\n",
    "    return {\"model\": best_model, \"accuracy\": accuracy_list, \"roc_auc\": roc_auc_list, \"recall\": recall_list, \"specificity\": specificity_list, \"fpr\": fpr, \"tpr\": tpr}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_model(results, X_test, y_test):\n",
    "    model = results[\"model\"]\n",
    "\n",
    "    selected_features = model.get_booster().feature_names\n",
    "    test_ds = X_test[selected_features]\n",
    "    print(f\"test_ds shape: {test_ds.shape}\")\n",
    "\n",
    "    y_pred = model.predict(test_ds)\n",
    "    print(y_pred)\n",
    "    y_pred_proba = model.predict_proba(test_ds)[:,1]\n",
    "\n",
    "    #draw the roc curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    specificity = tn / (tn+fp)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    \n",
    "    #return in percentage keep 2 decimal\n",
    "    return {\"accuracy\": accuracy*100, \"roc_auc\": roc_auc, \"f1\": f1*100, \"precision\": precision*100, \"recall\": recall*100, \"specificity\": specificity*100, \"fpr\": fpr, \"tpr\": tpr}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUC_results = {}\n",
    "for version, n_feature, color in zip(versions, n_features, colors):\n",
    "\n",
    "    print(f\"version: {version}\")\n",
    "    \n",
    "\n",
    "    filename = f\"Rdata/external_training_test_{version}.rda\"\n",
    "\n",
    "    training_test = pyreadr.read_r(filename)\n",
    "    train1 = training_test[\"um_data\"]\n",
    "    test1 = training_test[\"external_data\"]\n",
    "\n",
    "\n",
    "    print(f\"train1 shape: {train1.shape}, test1 shape: {test1.shape}\")\n",
    "\n",
    "    X1_train=train1.drop([\"Row.names\",\"ALS_status\"],axis=1)\n",
    "    y1_train=train1[\"ALS_status\"]\n",
    "    y1_train=y1_train.replace({'case':1,'control':0})\n",
    "\n",
    "    X1_test=test1.drop([\"Row.names\",\"ALS_status\"],axis=1)\n",
    "    y1_test=test1[\"ALS_status\"]\n",
    "    #count how many case and control in ALS_status\n",
    "    y1_test=y1_test.replace({'case':1,'control':0})\n",
    "\n",
    "    #whole\n",
    "    whole_results = cross_validate_xgboost_model(X1_train, y1_train, n_splits=10)\n",
    "    best_model_whole = whole_results[\"model\"]\n",
    "\n",
    "    thresholds = sort(best_model_whole.feature_importances_)\n",
    "    thresh= thresholds[-n_feature]\n",
    "\n",
    "    #internal\n",
    "    selection = SelectFromModel(estimator=best_model_whole, threshold=thresh, prefit=True)\n",
    "    col_index = X1_train.columns[selection.get_support()]\n",
    "    select_X_train = selection.transform(X1_train)\n",
    "    select_X_train = pd.DataFrame(select_X_train, columns=col_index)\n",
    "    print(f\"thresh: {thresh}, n={select_X_train.shape[1]}\")\n",
    "    # train model\n",
    "    internal_results = cross_validate_xgboost_model(select_X_train, y1_train, n_splits=10, suppress_output=True)\n",
    "    feature_importance = internal_results[\"model\"].get_booster().get_score(importance_type='weight')\n",
    "    feature_importance = pd.DataFrame(feature_importance.items(), columns=['feature', 'importance'])\n",
    "    internal_AUC = np.mean(internal_results[\"roc_auc\"])\n",
    "    internal_ROC = pd.DataFrame({\"fpr\":internal_results[\"fpr\"],\"tpr\":internal_results[\"tpr\"]})\n",
    "    print(f\"internal AUC:{internal_AUC}\")\n",
    "\n",
    "    AUC_results[version]={}\n",
    "    AUC_results[version][\"feature\"] = feature_importance\n",
    "    AUC_results[version][\"internal\"] = {\"AUC\":internal_AUC, \"ROC\":internal_ROC, \"label\":f\"internal {n_feature}\", \"color\":color}\n",
    "\n",
    "    #external\n",
    "    external_results = predict_model(internal_results, X1_test, y1_test)\n",
    "    external_AUC = external_results[\"roc_auc\"]\n",
    "    external_ROC = pd.DataFrame({\"fpr\":external_results[\"fpr\"],\"tpr\":external_results[\"tpr\"]})\n",
    "    print(f\"external AUC:{external_AUC}\")\n",
    "    AUC_results[version] [\"external\"] = {\"AUC\":external_AUC, \"ROC\":external_ROC, \"label\":f\"external {n_feature}\", \"color\":color}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "colors={1:\"#F28147\", 3:\"#9FD4AE\",4:\"#5560AC\"}\n",
    "\n",
    "for version in versions:\n",
    "    \n",
    "    #plot the internal\n",
    "    internal_roc = AUC_results[version][\"internal\"][\"ROC\"]\n",
    "    external_roc = AUC_results[version][\"external\"][\"ROC\"]\n",
    "    plt.plot(internal_roc[\"fpr\"], internal_roc[\"tpr\"], label=f\"{AUC_results[version]['internal']['label']}: {AUC_results[version]['internal']['AUC']*100:.2f}%\", color=colors[version], linestyle='--')\n",
    "    plt.plot(external_roc[\"fpr\"], external_roc[\"tpr\"], label=f\"{AUC_results[version]['external']['label']}: {AUC_results[version]['external']['AUC']*100:.2f}%\", color=colors[version])\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC curve')\n",
    "#add label\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for version in versions:\n",
    "    precision_recall_df = pd.read_csv(f\"precision_recall_curve_{version}.csv\")\n",
    "    plt.plot(precision_recall_df[\"recall\"], precision_recall_df[\"precision\"], label=f\"version {version}\", color=colors[version])\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall curve')\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.figure(figsize=(6, 6))\n",
    "for version in versions:\n",
    "    precision_recall_df = pd.read_csv(f\"precision_recall_curve_{version}.csv\")\n",
    "    plt.plot(precision_recall_df[\"recall\"], precision_recall_df[\"precision\"], label=f\"version {version}\", color=colors[version])\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall curve')\n",
    "plt.legend(loc=\"lower left\")\n",
    "\n",
    "\n",
    "\n",
    "plt.gca().set_aspect('equal', adjustable='box')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
